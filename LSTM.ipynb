{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshita-Kanal/Deep-Learning/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd76UR9iokOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su5Sy340v5aT",
        "colab_type": "text"
      },
      "source": [
        "### Idea behind LSTM\n",
        "Short term memory <br/> one to many relation: RNN helps make to sense out of the sentence <br/>\n",
        "many to one: sentiment analysis <br/>\n",
        "many to many: translators\n",
        "\n",
        "### Vanishing gradient\n",
        "Find optimal local minima of the cost function, gradient descent.\n",
        "In RNN, every node is not just a node but a representation of a whole later of nodes.Multiply o/p by weight to go to next layer, same weight, when we multiply by something small, value decreases quickly, weights are assigned at a random value, multiplying by many times, value would be lower, gradient keeps reducing,\n",
        "the lower the gradient is the slower the weights get updated, for a layer the weight gets updated several times.These layer's ouptuts are used as input for other layers, outputs are incorrect, because of this disbalance. \"vanishing gradient\", the entire network gets affected, \"domino effect\", There is also an exploding gradient problem.\n",
        "\n",
        "### Overcoming\n",
        "Trucating backpropagation, penalties, gradient clipping (maximum limit), weight initialization.\n",
        "\n",
        "### References\n",
        "\n",
        "http://proceedings.mlr.press/v28/pascanu13.pdf\n",
        "\n"
      ]
    }
  ]
}